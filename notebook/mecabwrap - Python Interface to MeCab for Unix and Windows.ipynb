{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mecabwrap\n",
    "## A Python Interface to MeCab for Unix and Windows\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr>    \n",
    "    <td>\n",
    "        <a href=\"https://travis-ci.org/kota7/mecabwrap-py\" target=\"_blank\">\n",
    "            <img src=\"https://travis-ci.org/kota7/mecabwrap-py.svg?branch=master\">\n",
    "        </a>\n",
    "    </td>    \n",
    "    <td>\n",
    "        <a href=\"https://ci.appveyor.com/project/kota7/mecabwrap-py/branch/master \" target=\"_blank\">\n",
    "            <img src=\"https://ci.appveyor.com/api/projects/status/oidn1rfte6u8kavs/branch/master?svg=true\">\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://badge.fury.io/py/mecabwrap\" target=\"_blank\">\n",
    "            <img src=\"https://badge.fury.io/py/mecabwrap.svg\">\n",
    "        </a>\n",
    "    </td>\n",
    "</tr>    \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mecabwrap** is yet another Python interface to [MeCab Morphological Analyzer](http://taku910.github.io/mecab/).\n",
    "\n",
    "It is designed to work seamlessly both on Unix and Windows machine.\n",
    "\n",
    "\n",
    "## Requirement\n",
    "\n",
    "- Python 2.7+ or 3.4+ (May also work on older versions, but not tested any more)\n",
    "- MeCab 0.996\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "\n",
    "### 1. Install MeCab\n",
    "\n",
    "**Ubuntu**\n",
    "\n",
    "```bash\n",
    "$ sudo apt-get install mecab libmecab-dev mecab-ipadic-utf8\n",
    "```\n",
    "\n",
    "**Mac OSX**\n",
    "\n",
    "```bash\n",
    "$ brew install mecab mecab-ipadic\n",
    "```\n",
    "\n",
    "**Windows**\n",
    "\n",
    "Download and run the [installer](https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7WElGUGt6ejlpVXc).\n",
    "\n",
    "See also: [official website](http://taku910.github.io/mecab/#install) \n",
    "\n",
    "\n",
    "\n",
    "### 2. Install this Package\n",
    "\n",
    "\n",
    "The package is now on [PyPI](https://pypi.python.org/pypi/mecabwrap/), so can be installed by `pip` command:\n",
    "\n",
    "```bash\n",
    "$ pip install mecabwrap\n",
    "```\n",
    "\n",
    "Or, the latest development version can be installed from the GitHub.\n",
    "\n",
    "```bash\n",
    "$ git clone --depth 1 https://github.com/kota7/mecabwrap-py.git\n",
    "$ cd mecabwrap-py\n",
    "$ pip install -U ./\n",
    "```\n",
    "\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "\n",
    "Following command will print the MeCab version.\n",
    "Otherwise, you do not have MeCab installed or MeCab is not on the search path.\n",
    "\n",
    "```bash\n",
    "$ mecab -v\n",
    "# should result in `mecab of 0.996` or similar.\n",
    "```\n",
    "\n",
    "\n",
    "To verify that the package is successfully installed, try the following:\n",
    "\n",
    "```bash\n",
    "$ python\n",
    "```\n",
    "\n",
    "```python\n",
    ">>> from mecabwrap import tokenize\n",
    ">>> for token in tokenize(u\"すもももももももものうち\"): \n",
    "...     print(token)\n",
    "... \n",
    "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
    "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
    "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
    "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
    "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
    "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
    "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecabwrap                     0.3.4       \n"
     ]
    }
   ],
   "source": [
    "# Version for this notebook\n",
    "!pip list | grep mecabwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "\n",
    "### A Simple Tokenizer\n",
    "\n",
    "The `tokenize` function is a high level API for splitting a text into tokens.\n",
    "It returns a generator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ,*,*\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ,*,*\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ,*,*\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ,*,*\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ,*,*\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ,*,*\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ,*,*\n"
     ]
    }
   ],
   "source": [
    "from mecabwrap import tokenize, print_token\n",
    "\n",
    "for token in tokenize('すもももももももものうち'):\n",
    "    print_token(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Token` is defined as a namedtuple (v0.3.2+) with the following fields:\n",
    "\n",
    "- `surface`: Word that appear in the text\n",
    "- `pos`: Part of speech\n",
    "- `pos1`: Part of speech, detail 1\n",
    "- `pos2`: Part of speech, detail 2\n",
    "- `pos3`: Part of speech, detail 3\n",
    "- `infl_type`: Inflection type\n",
    "- `infl_form`: Inflection form\n",
    "- `baseform`: Original form\n",
    "- `reading`: Surface written in katakana\n",
    "- `phoenetic`: Surface pronunciation\n",
    "- `lemma`: Representative form of the word. 語彙素\n",
    "- `lemma_reading`: Reading of lemma\n",
    "\n",
    "Among these, lemma and lemma_reading are not available in ipadic.  They are defined in unidic-based dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token(surface='うち', pos='名詞', pos1='非自立', pos2='副詞可能', pos3=None, infl_type=None, infl_form=None, baseform='うち', reading='ウチ', phoenetic='ウチ', lemma=None, lemma_reading=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MeCab Options\n",
    "\n",
    "To configure the MeCab calls, one may use `do_` functions that support arbitrary number of MeCab options.  \n",
    "Currently, the following three `do_` functions are provided.\n",
    "- `do_mecab`: works with a single input text and returns the result as a string.\n",
    "- `do_mecab_vec`: works with a multiple input texts and returns a string of concatenated results.\n",
    "- `do_mecab_iter`: works with a multiple input texts and returns a generator.\n",
    "\n",
    "For example, following code invokes the *wakati* option, so the outcome be words separated by spaces with no meta information. \n",
    "See [the official site](http://taku910.github.io/mecab/format.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人生 楽 ありゃ 苦 も ある さ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mecabwrap import do_mecab\n",
    "out = do_mecab('人生楽ありゃ苦もあるさ', '-Owakati')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exapmle below uses `do_mecab_vec` to parse multiple texts.\n",
    "Note that `-F` option configures the outcome formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春(一般) | は(係助詞) | あけぼの(固有名詞) | ...ここまで\n",
      "やうやう(一般) | 白い(自立) | なる(自立) | ゆく(非自立) | 山際(一般) | ...ここまで\n",
      "少し(助詞類接続) | 明かり(一般) | て(格助詞) | ...ここまで\n",
      "紫(一般) | だ() | ちる(自立) | たり() | 雲(一般) | の(連体化) | 細い(自立) | たなびく(自立) | たり() | ...ここまで\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mecabwrap import do_mecab_vec\n",
    "ins = ['春はあけぼの', 'やうやう白くなりゆく山際', '少し明かりて', '紫だちたる雲の細くたなびきたる']\n",
    "\n",
    "out = do_mecab_vec(ins, '-F%f[6](%f[1]) | ', '-E...ここまで\\n')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning Iterators\n",
    "\n",
    "When the number of input text is large, then holding the outcomes in the memory may not be a good idea.  `do_mecab_iter` function, which works for multiple texts, returns a generator of MeCab results.\n",
    "When `byline=True`, chunks are separated by line breaks; a chunk corresponds to a token in the default setting.\n",
    "When `byline=False`, chunks are separated by `EOS`; hence a chunk corresponds to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** generating tokens ***\n",
      "(1)\t春\t名詞,一般,*,*,*,*,春,ハル,ハル\n",
      "(2)\tは\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "(3)\tあけぼの\t名詞,固有名詞,地域,一般,*,*,あけぼの,アケボノ,アケボノ\n",
      "(4)\tEOS\n",
      "(5)\tやうやう\t副詞,一般,*,*,*,*,やうやう,ヤウヤウ,ヨーヨー\n",
      "(6)\t白く\t形容詞,自立,*,*,形容詞・アウオ段,連用テ接続,白い,シロク,シロク\n",
      "(7)\tなり\t動詞,自立,*,*,五段・ラ行,連用形,なる,ナリ,ナリ\n",
      "(8)\tゆく\t動詞,非自立,*,*,五段・カ行促音便ユク,基本形,ゆく,ユク,ユク\n",
      "(9)\t山際\t名詞,一般,*,*,*,*,山際,ヤマギワ,ヤマギワ\n",
      "(10)\tEOS\n",
      "(11)\t少し\t副詞,助詞類接続,*,*,*,*,少し,スコシ,スコシ\n",
      "(12)\t明かり\t名詞,一般,*,*,*,*,明かり,アカリ,アカリ\n",
      "(13)\tて\t助詞,格助詞,連語,*,*,*,て,テ,テ\n",
      "(14)\tEOS\n",
      "(15)\t紫\t名詞,一般,*,*,*,*,紫,ムラサキ,ムラサキ\n",
      "(16)\tだ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
      "(17)\tち\t動詞,自立,*,*,五段・ラ行,体言接続特殊２,ちる,チ,チ\n",
      "(18)\tたる\t助動詞,*,*,*,文語・ナリ,体言接続,たり,タル,タル\n",
      "(19)\t雲\t名詞,一般,*,*,*,*,雲,クモ,クモ\n",
      "(20)\tの\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "(21)\t細く\t形容詞,自立,*,*,形容詞・アウオ段,連用テ接続,細い,ホソク,ホソク\n",
      "(22)\tたなびき\t動詞,自立,*,*,五段・カ行イ音便,連用形,たなびく,タナビキ,タナビキ\n",
      "(23)\tたる\t助動詞,*,*,*,文語・ナリ,体言接続,たり,タル,タル\n",
      "(24)\tEOS\n",
      "\n",
      "*** generating tokenized sentences ***\n",
      "---(1)\n",
      "春\t名詞,一般,*,*,*,*,春,ハル,ハル\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "あけぼの\t名詞,固有名詞,地域,一般,*,*,あけぼの,アケボノ,アケボノ\n",
      "（文の終わり）\n",
      "---(2)\n",
      "やうやう\t副詞,一般,*,*,*,*,やうやう,ヤウヤウ,ヨーヨー\n",
      "白く\t形容詞,自立,*,*,形容詞・アウオ段,連用テ接続,白い,シロク,シロク\n",
      "なり\t動詞,自立,*,*,五段・ラ行,連用形,なる,ナリ,ナリ\n",
      "ゆく\t動詞,非自立,*,*,五段・カ行促音便ユク,基本形,ゆく,ユク,ユク\n",
      "山際\t名詞,一般,*,*,*,*,山際,ヤマギワ,ヤマギワ\n",
      "（文の終わり）\n",
      "---(3)\n",
      "少し\t副詞,助詞類接続,*,*,*,*,少し,スコシ,スコシ\n",
      "明かり\t名詞,一般,*,*,*,*,明かり,アカリ,アカリ\n",
      "て\t助詞,格助詞,連語,*,*,*,て,テ,テ\n",
      "（文の終わり）\n",
      "---(4)\n",
      "紫\t名詞,一般,*,*,*,*,紫,ムラサキ,ムラサキ\n",
      "だ\t助動詞,*,*,*,特殊・ダ,基本形,だ,ダ,ダ\n",
      "ち\t動詞,自立,*,*,五段・ラ行,体言接続特殊２,ちる,チ,チ\n",
      "たる\t助動詞,*,*,*,文語・ナリ,体言接続,たり,タル,タル\n",
      "雲\t名詞,一般,*,*,*,*,雲,クモ,クモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "細く\t形容詞,自立,*,*,形容詞・アウオ段,連用テ接続,細い,ホソク,ホソク\n",
      "たなびき\t動詞,自立,*,*,五段・カ行イ音便,連用形,たなびく,タナビキ,タナビキ\n",
      "たる\t助動詞,*,*,*,文語・ナリ,体言接続,たり,タル,タル\n",
      "（文の終わり）\n"
     ]
    }
   ],
   "source": [
    "from mecabwrap import do_mecab_iter\n",
    "\n",
    "ins = ['春はあけぼの', 'やうやう白くなりゆく山際', '少し明かりて', '紫だちたる雲の細くたなびきたる']\n",
    "\n",
    "print('\\n*** generating tokens ***')\n",
    "i = 0\n",
    "for text in do_mecab_iter(ins, byline=True):\n",
    "    i += 1\n",
    "    print('(' + str(i) + ')\\t' + text)\n",
    "    \n",
    "print('\\n*** generating tokenized sentences ***')\n",
    "i = 0\n",
    "for text in do_mecab_iter(ins, '-E', '（文の終わり）', byline=False):\n",
    "    i += 1\n",
    "    print('---(' + str(i) + ')\\n' + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the outcome to a file\n",
    "\n",
    "To write the MeCab outcomes directly to a file, one may either use `-o` option or `outpath` argument.  Note that this does not work with `do_mecab_iter`, since it is designed to write the outcomes to a temporary file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "EOS\n",
      "\n",
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "EOS\n",
      "\n",
      "`-o` option is not supported for `do_mecab_iter`\n",
      "`outpath` option is not supported for `do_mecab_iter`\n"
     ]
    }
   ],
   "source": [
    "do_mecab('すもももももももものうち', '-osumomo1.txt')\n",
    "# or,\n",
    "do_mecab('すもももももももものうち', outpath='sumomo2.txt')\n",
    "\n",
    "with open('sumomo1.txt') as f: \n",
    "    print(f.read())\n",
    "with open('sumomo2.txt') as f: \n",
    "    print(f.read())\n",
    "\n",
    "import os\n",
    "# clean up\n",
    "os.remove('sumomo1.txt')\n",
    "os.remove('sumomo2.txt')\n",
    "\n",
    "\n",
    "# these get error\n",
    "try:\n",
    "    res = do_mecab_iter(['すもももももももものうち'], '-osumomo3.txt')\n",
    "    next(res)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    res = do_mecab_iter(['すもももももももものうち'], outpath='sumomo3.txt')\n",
    "    next(res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dictionary (v0.3.0+)\n",
    "\n",
    "`do_` functions accepts `dictionary` option to specify the location of the system directory.\n",
    "`dictionary` can be either:\n",
    "\n",
    "- path to the system directory\n",
    "- sub-directory name under the mecab's default dicdir (note: `mecab-config` is required for this)\n",
    "\n",
    "This provides an intuitive syntax for using extended dictionaries such as [ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd) or [unidic-nelogd](https://github.com/neologd/mecab-unidic-neologd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Default ipadic ***\n",
      "メロン\t名詞,一般,*,*,*,*,メロン,メロン,メロン\n",
      "パン\t名詞,一般,*,*,*,*,パン,パン,パン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "たい\t助動詞,*,*,*,特殊・タイ,基本形,たい,タイ,タイ\n",
      "EOS\n",
      "\n",
      "*** With ipadic neologd ***\n",
      "メロンパン\t名詞,固有名詞,一般,*,*,*,メロンパン,メロンパン,メロンパン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "たい\t助動詞,*,*,*,特殊・タイ,基本形,たい,タイ,タイ\n",
      "EOS\n",
      "\n",
      "メロンパン\t名詞,固有名詞,一般,*,*,*,メロンパン,メロンパン,メロンパン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "たい\t助動詞,*,*,*,特殊・タイ,基本形,たい,タイ,タイ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this cell assumes that mecab-ipadic-neologd is already installed\n",
    "# otherwise, follow the instruction at https://github.com/neologd/mecab-ipadic-neologd\n",
    "print(\"*** Default ipadic ***\")\n",
    "print(do_mecab(\"メロンパンを食べたい\"))\n",
    "\n",
    "print(\"*** With ipadic neologd ***\")\n",
    "print(do_mecab(\"メロンパンを食べたい\", dictionary=\"mecab-ipadic-neologd\"))\n",
    "\n",
    "# this is equivalent to giving the path\n",
    "dicdir, = !mecab-config --dicdir\n",
    "print(do_mecab(\"メロンパンを食べたい\",\n",
    "               dictionary=os.path.join(dicdir, \"mecab-ipadic-neologd\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Long Input and Buffer Size (v0.2.3+)\n",
    "\n",
    "When input text is longer than the input buffer size (default: 8192), MeCab automatically split it into two \"sentences\", by inserting an extra EOS (and a few letters are lost around the separation point).\n",
    "As a result, `do_mecab_vec` and `do_mecab_iter` might produce output of length longer than the input.\n",
    "\n",
    "The `do_` functions provide two workarounds for this:\n",
    "1.  If the option `auto_buffer_size` is `True`, the `input-buffer-size` option is automatically adjusted to the level as large as covering all input text.  Note that it won't work when the input size exceeds the MeCab's maximum buffer size, `8192 * 640` ~ 5MB.\n",
    "1.  If the option `trancate` is `True`, input text is truncated so that they are covered by the input buffer size.\n",
    "\n",
    "Note that `do_mecab` does not have these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output would contain extra EOS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input buffer size = 8325\n",
      "output length = 2\n",
      "***\n",
      "End of the first element\n",
      "モ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "!\t名詞,サ変接続,*,*,*,*,*\n",
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "記号,一般,*,*,*,*,*\n",
      "EOS\n",
      "***\n",
      "Beginning of the second element\n",
      "記号,一般,*,*,*,*,*\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "!\t名詞,サ変接続,*,*,*,*,*\n",
      "すもも\t名詞,一般\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "x = 'すもももももももものうち!' * 225\n",
    "print(\"input buffer size =\", len(x.encode()))\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    res1 = list(do_mecab_iter([x]))\n",
    "# the text is split into two since it exceeds the input buffer size\n",
    "print(\"output length =\", len(res1))\n",
    "\n",
    "print('***\\nEnd of the first element')\n",
    "print(res1[0][-150:])\n",
    "\n",
    "print('***\\nBeginning of the second element')\n",
    "print(res1[1][0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length = 1\n",
      "***\n",
      "End of the first element\n",
      "も\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "!\t名詞,サ変接続,*,*,*,*,*\n",
      "EOS\n",
      "number of \"!\" = 225\n",
      "\n",
      "output length = 1\n",
      "***\n",
      "End of the first element\n",
      "モ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "!\t名詞,サ変接続,*,*,*,*,*\n",
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "記号,一般,*,*,*,*,*\n",
      "EOS\n",
      "number of \"!\" = 221\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "res2 = list(do_mecab_iter([x], auto_buffer_size=True))\n",
    "print(\"output length =\", len(res2))\n",
    "\n",
    "print('***\\nEnd of the first element')\n",
    "print(res2[0][-150:])\n",
    "\n",
    "# count the number of '!', to confirm all 223 repetitions are covered\n",
    "print('number of \"!\" =', len(re.findall(r'!', ''.join(res2))))\n",
    "\n",
    "print()\n",
    "res3 = list(do_mecab_iter([x], truncate=True))\n",
    "print(\"output length =\", len(res3))\n",
    "\n",
    "print('***\\nEnd of the first element')\n",
    "print(res3[0][-150:])\n",
    "\n",
    "# count the number of '!', to confirm some are not covered due to trancation\n",
    "print('number of \"!\" =', len(re.findall(r'!', ''.join(res3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing (v0.3.2+)\n",
    "\n",
    "`mecab_batch` function supports multiple text input.\n",
    "The function takes a list of strings and apply mecab tokenizer to each.\n",
    "The output is the list of tokenization outcomes.\n",
    "\n",
    "`mecab_batch_iter` function works the similarly but returns a generator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Token(surface='明日', pos='名詞', pos1='副詞可能', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='明日', reading='アシタ', phoenetic='アシタ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='は', pos='助詞', pos1='係助詞', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='は', reading='ハ', phoenetic='ワ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='晴れる', pos='動詞', pos1='自立', pos2=None, pos3=None, infl_type='一段', infl_form='基本形', baseform='晴れる', reading='ハレル', phoenetic='ハレル', lemma=None, lemma_reading=None),\n",
       "  Token(surface='か', pos='助詞', pos1='副助詞／並立助詞／終助詞', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='か', reading='カ', phoenetic='カ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='な', pos='助詞', pos1='終助詞', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='な', reading='ナ', phoenetic='ナ', lemma=None, lemma_reading=None)],\n",
       " [Token(surface='雨', pos='名詞', pos1='一般', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='雨', reading='アメ', phoenetic='アメ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='なら', pos='助動詞', pos1=None, pos2=None, pos3=None, infl_type='特殊・ダ', infl_form='仮定形', baseform='だ', reading='ナラ', phoenetic='ナラ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='読書', pos='名詞', pos1='サ変接続', pos2=None, pos3=None, infl_type=None, infl_form=None, baseform='読書', reading='ドクショ', phoenetic='ドクショ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='を', pos='助詞', pos1='格助詞', pos2='一般', pos3=None, infl_type=None, infl_form=None, baseform='を', reading='ヲ', phoenetic='ヲ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='しよ', pos='動詞', pos1='自立', pos2=None, pos3=None, infl_type='サ変・スル', infl_form='未然ウ接続', baseform='する', reading='シヨ', phoenetic='シヨ', lemma=None, lemma_reading=None),\n",
       "  Token(surface='う', pos='助動詞', pos1=None, pos2=None, pos3=None, infl_type='不変化型', infl_form='基本形', baseform='う', reading='ウ', phoenetic='ウ', lemma=None, lemma_reading=None)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mecabwrap import mecab_batch\n",
    "\n",
    "x = [\"明日は晴れるかな\", \"雨なら読書をしよう\"]\n",
    "mecab_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each string is converted into a list of `Token` objects.\n",
    "To obtain a more concise outcome, We can specify a converter function to the tokens as `format_func` option.\n",
    "`format_func` must be a function that takes a single `Token` object and returns the parsed outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['明日', 'は', '晴れる', 'か', 'な'], ['雨', 'だ', '読書', 'を', 'する', 'う']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use baseform if exists, otherwise surface\n",
    "mecab_batch(x, format_func=lambda x: x.baseform or x.surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter certain part-of-speeches by `pos_filter` option.\n",
    "More complex filtering can be achieved by `filter_func` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['明日', '晴れる'], ['雨', '読書', 'する']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_batch(x, format_func=lambda x: x.baseform or x.surface, pos_filter=(\"名詞\", \"動詞\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['明日'], ['だ', '読書', 'する']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_batch(x, format_func=lambda x: x.baseform or x.surface, \n",
    "            filter_func=lambda x: len(x.surface)==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn compatible transformer\n",
    "\n",
    "`MecabTokenizer` is a scikit-learn compatible transformer that applies `mecab_batch` to a list of string inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['明日', 'は', '晴れる', 'か', 'な'], ['雨', 'なら', '読書', 'を', 'しよ', 'う']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mecabwrap import MecabTokenizer\n",
    "\n",
    "tokenizer = MecabTokenizer(format_func=lambda x: x.surface)\n",
    "tokenizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>あれ</th>\n",
       "      <th>か</th>\n",
       "      <th>な</th>\n",
       "      <th>に</th>\n",
       "      <th>は</th>\n",
       "      <th>天気</th>\n",
       "      <th>明日</th>\n",
       "      <th>晴れる</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.499221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         あれ         か       な         に         は        天気      明日       晴れる\n",
       "0  0.000000  0.499221  0.3552  0.000000  0.499221  0.000000  0.3552  0.499221\n",
       "1  0.499221  0.000000  0.3552  0.499221  0.000000  0.499221  0.3552  0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "x = [\"明日は晴れるかな\", \"明日天気になあれ\"]\n",
    "\n",
    "p = Pipeline([\n",
    "    (\"mecab\", MecabTokenizer(format_func=lambda x: x.surface)),\n",
    "    (\"tfidf\", TfidfVectorizer(tokenizer=lambda x: x, lowercase=False))\n",
    "])\n",
    "\n",
    "y = p.fit_transform(x).todense()\n",
    "pd.DataFrame(y, columns=p.steps[-1][-1].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Python 2\n",
    "\n",
    "All text inputs are assumed to be unicode.  \n",
    "In Python2, inputs must be `u''` string, not `''`.\n",
    "In python3, `str` type is unicode, so `u''` and `''` are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "EOS\n",
      "\n",
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o1 = do_mecab('すもももももももものうち')   # this works only for python 3\n",
    "o2 = do_mecab(u'すもももももももものうち')  # this works both for python 2 and 3\n",
    "print(o1)\n",
    "print(o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on dictionary encodings\n",
    "\n",
    "The functions takes `mecab_enc` option, which indicates the encoding of the MeCab dictionary being used.  Usually this can be left as the default value `None`, so that the encoding is automatically detected.  Alternatively, one may specify the encoding explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charset:\tUTF-8\n",
      "\n",
      "日本\t名詞,固有名詞,地域,国,*,*,日本,ニッポン,ニッポン\n",
      "列島\t名詞,一般,*,*,*,*,列島,レットウ,レットー\n",
      "改造\t名詞,サ変接続,*,*,*,*,改造,カイゾウ,カイゾー\n",
      "論\t名詞,接尾,一般,*,*,*,論,ロン,ロン\n",
      "EOS\n",
      "\n",
      "日本\t名詞,固有名詞,地域,国,*,*,日本,ニッポン,ニッポン\n",
      "列島\t名詞,一般,*,*,*,*,列島,レットウ,レットー\n",
      "改造\t名詞,サ変接続,*,*,*,*,改造,カイゾウ,カイゾー\n",
      "論\t名詞,接尾,一般,*,*,*,論,ロン,ロン\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show mecab dict\n",
    "! mecab -D | grep charset\n",
    "print()\n",
    "\n",
    "o1 = do_mecab('日本列島改造論', mecab_enc=None)      # default\n",
    "print(o1)\n",
    "\n",
    "o2 = do_mecab('日本列島改造論', mecab_enc='utf-8')   # explicitly specified\n",
    "print(o2)\n",
    "\n",
    "#o3 = do_mecab('日本列島改造論', mecab_enc='cp932')   # wrong encoding, fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
